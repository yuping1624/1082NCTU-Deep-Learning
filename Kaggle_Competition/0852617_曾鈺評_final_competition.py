# -*- coding: utf-8 -*-
"""0852617_曾鈺評_Final_Competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ctqWkyfDazq3kTWpeGDaQ1clWF0AdcVC
"""

import numpy as np 
import pandas as pd 
import jieba
import jieba.posseg as pseg
import jieba.analyse
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import keras
from keras import layers, Input
from keras.layers import Embedding, LSTM, concatenate, Dense, Dropout
from keras.models import Sequential, Model
from keras.utils import plot_model

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

jieba.enable_parallel(4)

train_data = pd.read_csv('/kaggle/input/dl-course-final-competition/train_data.csv')
train_data

sample_data = pd.read_csv('/kaggle/input/dl-course-final-competition/sample.csv')
sample_data

test_data = pd.read_csv('/kaggle/input/dl-course-final-competition/test_data.csv')
test_data

groups = train_data.groupby('label')
groups.agg(np.size)

test = test_data['keyword'].astype(str)[0]
words = pseg.cut(test)
words2 = ' '.join([word for word, flag in words if (word != 'nan') and (flag != 'x')])
words2

def jieba_tokenizer_nan(text):
    words = pseg.cut(text)
    return ' '.join([word for word, flag in words if (word != 'nan') and (flag != 'x')])

def jieba_tokenizer(text):
    words = pseg.cut(text)
    return ' '.join([word for word, flag in words if flag != 'x'])

train_data['title_tokenized'] = train_data.loc[:, 'title'].apply(jieba_tokenizer_nan)

train_data['keyword_tokenized'] = train_data['keyword'].astype(str).apply(jieba_tokenizer_nan)

test_data['title_tokenized'] = test_data.loc[:,'title'].apply(jieba_tokenizer_nan)

test_data['keyword_tokenized'] = test_data['keyword'].astype(str).apply(jieba_tokenizer_nan)

test_data.loc[0,'keyword_tokenized']

test_data.iloc[:,:].head()

#train_data.iloc[:,:].head()
train_data.to_csv('train_data_tokenized_v3.csv')

test_data.to_csv('test_data_tokenized_v3.csv')

MAX_NUM_WORDS = 500000
tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)

corpus_x1 = train_data.title_tokenized
corpus_x2 = train_data.keyword_tokenized
corpus = pd.concat([corpus_x1, corpus_x2])
print(corpus.shape)

tokenizer.fit_on_texts(corpus)

corpus.tail(10)

pd.DataFrame(corpus.iloc[-20:],
             columns=['title'])

x1_train = tokenizer.texts_to_sequences(corpus_x1)
x2_train = tokenizer.texts_to_sequences(corpus_x2)
len(x1_train)

x2_train[-6:]

for seq in x1_train[-1:]:
    print([tokenizer.index_word[idx] for idx in seq])

max_seq_len = max([len(seq) for seq in x1_train])
max_seq_len

MAX_SEQUENCE_LENGTH = 78
x1_train = keras.preprocessing.sequence.pad_sequences(x1_train, maxlen=MAX_SEQUENCE_LENGTH)
x2_train = keras.preprocessing.sequence.pad_sequences(x2_train, maxlen=MAX_SEQUENCE_LENGTH)

np.max(x1_train)

y_train = train_data['label']
y_train = np.asarray(y_train).astype('float32')
y_train = keras.utils.to_categorical(y_train)
y_train[-5:]

VALIDATION_RATIO = 0.1
RANDOM_STATE = 624
x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(x1_train, x2_train, y_train, test_size=VALIDATION_RATIO, random_state=RANDOM_STATE)

print(x1_train.shape, x2_train.shape, x1_val.shape, y_train.shape)

# 基本參數設置，有幾個分類
NUM_CLASSES = 10

# 在語料庫裡有多少詞彙
MAX_NUM_WORDS =500000

# 一個標題最長有幾個詞彙
MAX_SEQUENCE_LENGTH = 78

# 一個詞向量的維度
NUM_EMBEDDING_DIM = 1024

# LSTM 輸出的向量維度
NUM_LSTM_UNITS = 128

# construct LSTM


# define input
top_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')
bm_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')

# word embedding
embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)
top_embedded = embedding_layer(top_input)
bm_embedded = embedding_layer(bm_input)

# LSTM (transfer to 128 dim vector)
shared_lstm = LSTM(NUM_LSTM_UNITS)
top_output = shared_lstm(top_embedded)
bm_output = shared_lstm(bm_embedded)

merged = concatenate([top_output, bm_output], axis=-1)

# fully connected
dense =  Dense(units=NUM_CLASSES, activation='softmax')
predictions = dense(merged)


model = Model(inputs=[top_input, bm_input], outputs=predictions)

plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='LR')

model.summary()

#opt = keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])  #'rmsprop'

BATCH_SIZE = 512
NUM_EPOCHS = 3

model.fit(
    x=[x1_train, x2_train], 
    y=y_train, batch_size=BATCH_SIZE, 
    epochs=NUM_EPOCHS,
    validation_data=([x1_val, x2_val],y_val),
    shuffle=True
)

model.save('model_LSTM')

model.history.history

x_lim = np.linspace(0,1, NUM_EPOCHS)
plt.plot(model.history.history['accuracy'], label='train')
plt.plot(model.history.history['val_accuracy'], label='test')
plt.title('learning rate')
plt.savefig('image_accuracy.png')
plt.show()

x1_test = tokenizer.texts_to_sequences(test_data.title_tokenized)
x2_test = tokenizer.texts_to_sequences(test_data.keyword_tokenized)
x1_test = keras.preprocessing.sequence.pad_sequences(x1_test,maxlen=MAX_SEQUENCE_LENGTH)
x2_test = keras.preprocessing.sequence.pad_sequences(x2_test,maxlen=MAX_SEQUENCE_LENGTH)

predictions = model.predict([x1_test,x2_test])

predictions

test_data['label'] = np.argmax(predictions, axis=1)
test_data

submission = test_data.loc[:,['id', 'label']]
submission.to_csv('submission13.csv', index=False)

submission

